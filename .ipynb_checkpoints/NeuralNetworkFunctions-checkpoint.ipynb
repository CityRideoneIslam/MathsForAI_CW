{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d320c960-a9c3-45ff-a465-5db2181c6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc42fac-dc94-4bd1-a610-cf1f59a14a14",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "## Sigmoid function\n",
    "The sigmoid function is used as an activation function because it squashes the output to a probability value between 0 and 1, which is useful when the output is a probability or binary; hence, it is commonly used in binary classification models. The function also allows the network to learn more complex decision bondaries. The formula for the sigmoid function is $$ σ(x) = \\frac{1}{1 + e^{-x}}. $$\n",
    "## Derivative of sigmoid\n",
    "Back propagation is essential to calculate the grandient of the loss function with respect to the weights and biases in a neural network. It allows the netowrk to effectively learn from its errors and adapt its weights based on the activating functions to update. The backward pass for sigmoid is the deravative of the sigmoid function, which can be mathematically expressed as $$ σ'(x) = σ(x) \\cdot \\bigl(1 - σ(x)\\bigr) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960b0abe-f743-437d-b006-c9e4ae625d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def backward(x):\n",
    "        return sigmoid_forward(x) * (1 - sigmoid_forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95c1b2-5580-4f77-8a80-d411542ecbf2",
   "metadata": {},
   "source": [
    "## Tanh function\n",
    "The output for the tanh function is symmetric around the origin, which can help learning alorithms converge. This function outperforms the sigmoid function in multi-layer neural networks. The formula for the tanh function can be expressed as $$ tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\n",
    "## Derivative of tanh\n",
    "Similarly to the backward pass of the sigmoid function, the backward pass of the tanh function is the derivate of it, which can be expressed as $$tanh'(x){dx} = 1 - tanh(x)^{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3e2a9b-de82-488a-ada9-e06103d3047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    \n",
    "    def backward(x):\n",
    "        return 1 - (forward_tanh(x) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2f8be-3737-42a7-84d9-40029ce93927",
   "metadata": {},
   "source": [
    "## ReLU function\n",
    "The ReLU (Rectified Linear Unit) function helps the model learn more complex relationships in data and makes accurate predictions, and it's computationally efficient, due to its non-linearity. The ReLU function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}(x) = \n",
    "\\begin{cases} \n",
    "x, & \\text{if } x \\geq 0 \\\\ \n",
    "0, & \\text{if } x < 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "## Derivative of ReLU\n",
    "The backward pass for the relu function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}'(x) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } x > 0 \\\\ \n",
    "0, & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba63e52-7deb-4056-b224-2cb5776ee796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def forward(x):\n",
    "        return np.maximum(0, x)\n",
    "        \n",
    "    def backward(x):\n",
    "        return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8d8f3-dc1c-4fc1-8514-630440b64fb5",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "## mean Squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5661d388-baa2-4305-a029-30675a7bf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "class MSE:\n",
    "    def forward(self, y_true, y_pred):\n",
    "        return np.mean(np.power(y_true-y_pred, 2));\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d6d99-02bd-4e02-92a5-7f686e86662d",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "Unlike the sigmoid function, the softmax function is used in multiclass classification tasks: the function converts the output into probabilities, where the probability represents the likelihood of the input being in each class.The softmax function can mathematically be expressed as $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$\n",
    "## Derivative of Softmax\n",
    "$$\n",
    "softmax'(z_i) = \\text{softmax}(z_i) \\cdot (\\delta_{ik} - \\text{softmax}(z_k))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } \\delta_{ik} = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } i = k \\\\ \n",
    "0, & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a40bd61-edcb-44ee-8310-279531a085b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def softmax_forward(vector):\n",
    "        e = np.exp(vector)\n",
    "        return e / np.sum(e)\n",
    "    def softmax_backward(vector, y):\n",
    "        p = softmax_forward(vector)\n",
    "        return p - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ce958-ab6a-4292-93d2-6b3df51220da",
   "metadata": {},
   "source": [
    "# Dropout function\n",
    "Dropout prevents overfitting and regularises by randomly \"dropping\" connections between neurons in successive layers when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "117dcb82-b5b2-414b-a5cb-ecdcc1a9d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, dropout_rate, training=True):\n",
    "    if training:\n",
    "        mask = np.random.rand(*X.shape) < (1 - dropout_rate)\n",
    "        X = X * mask / (1 - dropout_rate)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d6a00-2f36-4093-9992-63077fa6aa5c",
   "metadata": {},
   "source": [
    "# Implemented Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7974729b-d864-4385-a325-2683d23f834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c8e2db1-c994-4612-b91d-5a41a351f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        if self.activation == \"sigmoid\":\n",
    "            self.output = Sigmoid.forward(self.input)\n",
    "        elif self.activation == \"tanh\":\n",
    "            self.output = Tanh.forward(self.input)\n",
    "        elif self.activation == \"relu\":\n",
    "            self.output = Relu.forward(self.input)\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return Sigmoid.backward(self.input) * output_error\n",
    "        elif self.activation == \"tanh\":\n",
    "            return Tanh.backward(self.input) * output_error\n",
    "        elif self.activation == \"relu\":\n",
    "            return Relu.backward(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07c039d8-361c-4de1-b7c3-c7f478f42f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, learning_rate, epochs):\n",
    "        dims = len(X)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(dims):\n",
    "                output = X[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                err += MSE.forward(y[j], output)\n",
    "\n",
    "                error += MSE.backward(y[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "                err /= dims\n",
    "                print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "\n",
    "    def predict(self, test):\n",
    "        dims = len(test)\n",
    "        results = []\n",
    "\n",
    "        for i in range(dims):\n",
    "            output = test[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            results.append(output)\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f94697e-501a-45e2-8bff-90fc2f19a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8ecd0f2-8923-4601-b2c6-34df7b6e0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5ea08a9-556d-4a34-a349-b5725712be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0:\"airplane\",\n",
    "    1:\"automobile\",\n",
    "    2:\"bird\",\n",
    "    3:\"cat\",\n",
    "    4:\"deer\",\n",
    "    5:\"dog\",\n",
    "    6:\"frog\",\n",
    "    7:\"horse\",\n",
    "    8:\"ship\",\n",
    "    9:\"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c25fc952-9bd9-4306-a42b-3cc11baff376",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ccc45dc1-77c7-4dc0-8112-170196a930c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m net\u001b[38;5;241m.\u001b[39madd(Layer(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m))\n\u001b[0;32m     11\u001b[0m net\u001b[38;5;241m.\u001b[39madd(ActivationLayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 12\u001b[0m net\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self, X, y, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m X[j]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 18\u001b[0m     output \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward_propagation(output)\n\u001b[0;32m     20\u001b[0m err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m MSE\u001b[38;5;241m.\u001b[39mforward(y[j], output)\n\u001b[0;32m     23\u001b[0m error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m MSE\u001b[38;5;241m.\u001b[39mbackward(y[j], output)\n",
      "Cell \u001b[1;32mIn[84], line 8\u001b[0m, in \u001b[0;36mLayer.forward_propagation\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m input_data\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(input_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork()\n",
    "net.add(Layer(3, 1024))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(1024, 1024))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(1024, 512))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(512, 256))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(256, 128))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.fit(x_train, y_train, epochs=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40faf600-31b5-477c-9460-16a64472dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of x_train before flattening:\", x_train.shape)\n",
    "print(\"Shape of x_train after flattening:\", x_train_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138a47e-d2be-48fa-b91d-66f434eb947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec681243-9f96-429f-ac6c-9f4379a92e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
