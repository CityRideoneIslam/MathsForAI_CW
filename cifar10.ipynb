{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4f2a24-7e5b-4daf-955d-967793f24d1e",
   "metadata": {},
   "source": [
    "# A. Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536b5dc-1637-4c8a-a6a4-39fd5b6ebae7",
   "metadata": {},
   "source": [
    "- Name: CIFAR-10\n",
    "- Description: CIFAR-10 is a collection of 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n",
    "- Classes: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck.\n",
    "- Motivation: The dataset is ideal for learning as it is small enough to train within a reasonable time and complex enough to require meaningful architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b274993-e6dc-4022-93d7-0625e6c8630a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cifar10\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# DO NOT EDIT. Generated by api_gen.sh\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTypePolicy\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatDTypePolicy\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\api\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\api\\activations\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\activations\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\activations\\activations.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\__init__.py:9\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# When using the torch backend,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# torch needs to be imported first, otherwise it will segfault\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# upon import.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_utils\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutocastScope\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasVariable\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\dtypes.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[0;32m      7\u001b[0m BOOL_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[0;32m      8\u001b[0m INT_TYPES \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstateless_scope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_stateless_scope\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstateless_scope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_stateless_scope\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasVariable\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_dataset_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_dataset_from_directory\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_file\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\audio_dataset_utils.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_utils\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_io \u001b[38;5;28;01mas\u001b[39;00m tfio\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPool\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\tree\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_same_structure\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flatten\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_nested\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\tree\\tree_api.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optree\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optree\u001b[38;5;241m.\u001b[39mavailable:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dmtree\u001b[38;5;241m.\u001b[39mavailable:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dmtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\tree\\optree_impl.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_structures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[0;32m     19\u001b[0m     optree\u001b[38;5;241m.\u001b[39mregister_pytree_node(\n\u001b[0;32m     20\u001b[0m         ListWrapper,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: (x, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m metadata, children: ListWrapper(\u001b[38;5;28mlist\u001b[39m(children)),\n\u001b[0;32m     23\u001b[0m         namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_nested\u001b[39m(structure):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py:75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sysconfig\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tpu\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\tpu\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.tpu namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbfloat16\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16_scope \u001b[38;5;66;03m# line: 69\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_replica_sum \u001b[38;5;66;03m# line: 91\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c286e-ecf1-46a0-a602-084139b56450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad33725-4866-454d-be75-7a3eb03eda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0:\"airplane\",\n",
    "    1:\"automobile\",\n",
    "    2:\"bird\",\n",
    "    3:\"cat\",\n",
    "    4:\"deer\",\n",
    "    5:\"dog\",\n",
    "    6:\"frog\",\n",
    "    7:\"horse\",\n",
    "    8:\"ship\",\n",
    "    9:\"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27f27d-86f7-4262-a74b-532f3c0c8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae375b9c-a04b-4fbe-ab69-6671136cf679",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f748ee4-1e32-4bd5-9b73-5f96aca911e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[y_train[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(labels[y_train[i][0]])\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e5072-313a-4144-9fe3-26bc967773f9",
   "metadata": {},
   "source": [
    "# B. Sigmoid and ReLU Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f1a0b",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "The sigmoid function is used as an activation function because it squashes the output to a probability value between 0 and 1, which is useful when the output is a probability or binary; hence, it is commonly used in binary classification models. The function also allows the network to learn more complex decision bondaries. The formula for the sigmoid function is $$ σ(x) = \\frac{1}{1 + e^{-x}}. $$\n",
    "## Derivative of sigmoid\n",
    "Back propagation is essential to calculate the grandient of the loss function with respect to the weights and biases in a neural network. It allows the netowrk to effectively learn from its errors and adapt its weights based on the activating functions to update. The backward pass for sigmoid is the deravative of the sigmoid function, which can be mathematically expressed as $$ σ'(x) = σ(x) \\cdot \\bigl(1 - σ(x)\\bigr) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ab71b-dbd4-44a6-8a76-99fa77e1ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "       self.output = 1 / (1 + np.exp(-x))\n",
    "       return self.output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc86005-3c1e-4ee3-89e4-1ac84aea32a6",
   "metadata": {},
   "source": [
    "## ReLU function\n",
    "The ReLU (Rectified Linear Unit) function helps the model learn more complex relationships in data and makes accurate predictions, and it's computationally efficient, due to its non-linearity. The ReLU function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}(x) = \n",
    "\\begin{cases} \n",
    "x, & \\text{if } x \\geq 0 \\\\ \n",
    "0, & \\text{if } x < 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "## Derivative of ReLU\n",
    "The backward pass for the relu function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}'(x) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } x > 0 \\\\ \n",
    "0, & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a991b-7c3d-48df-9b88-5057b20614b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.maximum(0, x)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * (self.input > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb9932-18cd-42de-8ff8-b56eca845d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input data\n",
    "x = np.array([[0.5, -1.0], [2.0, -0.5]])\n",
    "\n",
    "# Sigmoid forward pass\n",
    "sigmoid_layer = Sigmoid()\n",
    "sigmoid_output = sigmoid_layer.forward(x)\n",
    "print(\"Sigmoid Forward Output:\\n\", sigmoid_output)\n",
    "\n",
    "# ReLU forward pass\n",
    "relu_layer = ReLU()\n",
    "relu_output = relu_layer.forward(x)\n",
    "print(\"ReLU Forward Output:\\n\", relu_output)\n",
    "\n",
    "# Example gradient from next layer\n",
    "grad = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "\n",
    "# Sigmoid backward pass\n",
    "sigmoid_grad = sigmoid_layer.backward(grad)\n",
    "print(\"Sigmoid Backward Gradient:\\n\", sigmoid_grad)\n",
    "\n",
    "# ReLU backward pass\n",
    "relu_grad = relu_layer.backward(grad)\n",
    "print(\"ReLU Backward Gradient:\\n\", relu_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2f9f0-1511-4fd2-8a9b-df771094b911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Sigmoid\n",
    "x = np.linspace(-10, 10, 100)\n",
    "sigmoid_output = 1 / (1 + np.exp(-x))\n",
    "sigmoid_derivative = sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, sigmoid_output, label=\"Sigmoid\")\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, sigmoid_derivative, label=\"Sigmoid Derivative\")\n",
    "plt.title(\"Sigmoid Derivative\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot ReLU\n",
    "relu_output = np.maximum(0, x)\n",
    "relu_derivative = (x > 0).astype(float)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, relu_output, label=\"ReLU\")\n",
    "plt.title(\"ReLU Function\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, relu_derivative, label=\"ReLU Derivative\")\n",
    "plt.title(\"ReLU Derivative\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d7e01",
   "metadata": {},
   "source": [
    "# C. Softmax Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114ca74",
   "metadata": {},
   "source": [
    "## Softmax function\r\n",
    "Unlike the sigmoid function, the softmax function is used in multiclass classification tasks: the function converts the output into probabilities, where the probability represents the likelihood of the input being in each class.The softmax function can mathematically be expressed as $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$\r\n",
    "## Derivative of Softmax\r\n",
    "The derivative of the softmax function can be combined with the cross-entropy loss and expressed as $$ \\frac{\\delta L}{\\delta z_i} = softmax(z_i) - y_i $$\r\n",
    "\r\n",
    "Where $L$ is the cross-entropy loss and can be expressed as\r\n",
    "\r\n",
    "$$\r\n",
    "L = -\\sum_i y_i log(softmax(z_i))\r\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fb148-ccdf-41f5-88f2-a80affaf033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward(logits):\n",
    "    shifted_logits = logits - np.max(logits, axis=1, keepdims=True)  # Stability fix\n",
    "    exp_logits = np.exp(shifted_logits)\n",
    "    probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "# Softmax Backward Function\n",
    "def softmax_backward(probabilities, labels):\n",
    "    grad_logits = probabilities - labels\n",
    "    return grad_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331d7c0-45af-4df6-b282-1f80ee880699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the first 10 examples from the training set for testing\n",
    "batch_size = 10\n",
    "num_classes = 10\n",
    "\n",
    "# Generate dummy logits for testing\n",
    "np.random.seed(42)  # For reproducibility\n",
    "logits = np.random.randn(batch_size, num_classes)  # Random logits for 10 images\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = np.zeros((batch_size, num_classes))\n",
    "for i in range(batch_size):\n",
    "    y_one_hot[i, y_train[i][0]] = 1\n",
    "\n",
    "# Forward pass (softmax probabilities)\n",
    "probabilities = softmax_forward(logits)\n",
    "\n",
    "# Backward pass (gradients w.r.t logits)\n",
    "gradients = softmax_backward(probabilities, y_one_hot)\n",
    "\n",
    "# Display some of the training images with their labels\n",
    "fig, ax = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    ax.imshow(x_train[i])\n",
    "    ax.set_title(f\"Label: {y_train[i][0]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"Softmax Probabilities:\\n\", probabilities)\n",
    "print(\"\\nGradients w.r.t logits:\\n\", gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3845f-744b-4671-9ca4-3f7528561595",
   "metadata": {},
   "source": [
    "# D. Dropout\r\n",
    "Dropout prevents overfitting and regularises by randomly \"dropping\" connections between neurons in successive layers when training. This regularises the neural network, preventing overfitting, forcing the network to learn redundant representations, which improves generalisation. Firstly, we need to generate a mask: \r\n",
    "$$ \r\n",
    "m_{ij} =  \r\n",
    "\\begin{cases}\r\n",
    "1 \\text{ with probability } (1-p)\\\\\r\n",
    "0 \\text{ with probability } p\r\n",
    "\\end{cases}\r\n",
    "$$\r\n",
    "where $p$ represent the dropout rate: the probability of dropping a neuron by setting it to zero. $m$ is a binary matrix with the same shape as $x$. Then we apply the mask and scale, and we get the output $y$:\r\n",
    "\r\n",
    "$$ y_{ij}=\\frac{x_{ij}\\cdot{m_{ij}}}{1-p} $$\r\n",
    "The backward function (derivative), can instead be expressed as \r\n",
    "$$ \\frac{\\delta L}{\\delta x_{ij}} = \\frac{\\delta L}{\\delta y_{ij}} \\cdot{\\frac{m_{ij}}{1-p}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e659515-595b-4e50-a6a0-d5aaf3755e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedDropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*x.shape) > self.dropout_rate).astype(float)\n",
    "            self.output = x * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            self.output = x\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * self.mask / (1 - self.dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06505313-eb14-41cd-bca5-6072403338b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "dropout_layer = InvertedDropout(dropout_rate=0.5)\n",
    "\n",
    "# Forward pass during training\n",
    "output_train = dropout_layer.forward(x, training=True)\n",
    "print(\"Output during training:\\n\", output_train)\n",
    "\n",
    "# Forward pass during test\n",
    "output_test = dropout_layer.forward(x, training=False)\n",
    "print(\"Output during testing:\\n\", output_test)\n",
    "\n",
    "grad = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "\n",
    "# Backward pass\n",
    "grad_back = dropout_layer.backward(grad)\n",
    "print(\"Gradient after backward pass:\\n\", grad_back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255e057-bcd2-45ab-8810-03cfc07b61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1000)\n",
    "\n",
    "# Apply dropout\n",
    "dropout_rates = [0.0, 0.3, 0.5, 0.8]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for rate in dropout_rates:\n",
    "    dropout_layer = InvertedDropout(dropout_rate=rate)\n",
    "    output = dropout_layer.forward(x, training=True)\n",
    "    plt.hist(output, bins=50, alpha=0.6, label=f\"Dropout rate = {rate}\")\n",
    "\n",
    "plt.title(\"Effect of Dropout on Activations\")\n",
    "plt.xlabel(\"Activation Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e73b7",
   "metadata": {},
   "source": [
    "# F. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f4f9b",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "Stochastic Gradient Descent (SGD) is a variant of gradient descent that updates the model's parameters using only a single or a few training examples at each step, rather than the full batch. This introduces noise in the optimization process, which can help escape local minima and speed up convergence. The rule is $$ θ=θ−η⋅∇J(θ) $$\n",
    "\n",
    "## SGD with Momentum\n",
    "SGD with momentum is a variation of the standard SGD that accelerates gradients vectors in the right directions, thus leading to faster converging. It introduces a \"velocity\" term that accumulates the gradient of the previous steps and helps overcome oscillations. The rule is $$ θ=θ−η⋅v \n",
    "t $$\n",
    "$$ v \n",
    "t\n",
    " =βv \n",
    "t−1\n",
    " +(1−β)∇J(θ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5578dfd-1366-4656-8824-adbc9595a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SGD Optimizer\n",
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, grads, params):\n",
    "        return params - self.learning_rate * grads\n",
    "\n",
    "# SGD with Momentum Optimizer\n",
    "class SGDWithMomentum:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def update(self, grads, params):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = {key: np.zeros_like(val) for key, val in params.items()}\n",
    "        \n",
    "        for key in params:\n",
    "            self.velocity[key] = self.momentum * self.velocity[key] + self.learning_rate * grads[key]\n",
    "     \n",
    "            params[key] -= self.velocity[key]\n",
    "        \n",
    "        return params  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea1e68-ff89-489f-b08d-9f716a6e3aba",
   "metadata": {},
   "source": [
    "# E. Fully Parametrizable NN class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334f2ec-9130-4617-9871-996578b6096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([i.flatten() for i in x_train]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5f86d-3c97-4077-8cb4-56a6cd713778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activations, dropout_rates=None, regularizer=None, lambda_reg=0.01):\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.dropout_rates = dropout_rates or [0.0] * (len(layers) - 1)\n",
    "        self.regularizer = regularizer\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.params = {}\n",
    "        self.cache = {}\n",
    "        self.dropout_masks = {}\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        for i in range(1, len(layers)):\n",
    "            self.params[f\"W{i}\"] = np.random.randn(layers[i-1], layers[i]) * np.sqrt(2 / layers[i-1])\n",
    "            self.params[f\"b{i}\"] = np.zeros((1, layers[i]))\n",
    "\n",
    "    def activation_forward(self, Z, activation):\n",
    "        if activation == \"relu\":\n",
    "            return np.maximum(0, Z)\n",
    "        elif activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-Z))\n",
    "        elif activation == \"softmax\":\n",
    "            exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "            return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def activation_backward(self, dA, Z, activation):\n",
    "        \"\"\"Compute gradient of activation function.\"\"\"\n",
    "        if activation == \"relu\":\n",
    "            dZ = dA * (Z > 0)\n",
    "            return dZ\n",
    "        elif activation == \"sigmoid\":\n",
    "            A = 1 / (1 + np.exp(-Z))\n",
    "            return dA * A * (1 - A)\n",
    "        elif activation == \"softmax\":\n",
    "            # For softmax, we assume dA is already the correct gradient\n",
    "            # as we're using cross-entropy loss\n",
    "            return dA\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.cache[\"A0\"] = X\n",
    "        A = X\n",
    "\n",
    "        for i in range(1, len(self.layers)):\n",
    "            W, b = self.params[f\"W{i}\"], self.params[f\"b{i}\"]\n",
    "            Z = np.dot(A, W) + b\n",
    "            self.cache[f\"Z{i}\"] = Z\n",
    "            A = self.activation_forward(Z, self.activations[i-1])\n",
    "            self.cache[f\"A{i}\"] = A\n",
    "\n",
    "            if training and self.dropout_rates[i-1] > 0:\n",
    "                mask = (np.random.rand(*A.shape) > self.dropout_rates[i-1]).astype(float)\n",
    "                A = A * mask / (1 - self.dropout_rates[i-1])\n",
    "                self.dropout_masks[f\"D{i}\"] = mask\n",
    "                self.cache[f\"A{i}\"] = A  # Update cache with dropped out activations\n",
    "\n",
    "        return A\n",
    "\n",
    "    def compute_loss(self, Y, predictions):\n",
    "        m = Y.shape[0]\n",
    "        cross_entropy = -np.sum(Y * np.log(predictions + 1e-8)) / m\n",
    "        \n",
    "        # Add regularization loss if applicable\n",
    "        reg_loss = 0\n",
    "        if self.regularizer:\n",
    "            for i in range(1, len(self.layers)):\n",
    "                W = self.params[f\"W{i}\"]\n",
    "                if self.regularizer == \"l2\":\n",
    "                    reg_loss += np.sum(np.square(W))\n",
    "                elif self.regularizer == \"l1\":\n",
    "                    reg_loss += np.sum(np.abs(W))\n",
    "            reg_loss *= (self.lambda_reg / (2 * m))\n",
    "            \n",
    "        return cross_entropy + reg_loss\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        grads = {}\n",
    "        \n",
    "        # Initialize gradient of the loss with respect to the output layer\n",
    "        dA = self.cache[f\"A{len(self.layers)-1}\"] - Y\n",
    "\n",
    "        for i in reversed(range(1, len(self.layers))):\n",
    "            Z = self.cache[f\"Z{i}\"]\n",
    "            A_prev = self.cache[f\"A{i-1}\"]\n",
    "            W = self.params[f\"W{i}\"]\n",
    "\n",
    "            dropout_mask = self.dropout_masks.get(f\"D{i}\", None)\n",
    "\n",
    "            # Compute dZ using the proper activation gradient\n",
    "            if i == len(self.layers)-1:\n",
    "                dZ = dA  \n",
    "            else:\n",
    "                dZ = self.activation_backward(dA, Z, self.activations[i-1])\n",
    "                \n",
    "                # Apply dropout mask to dZ (not dA)\n",
    "                if dropout_mask is not None:\n",
    "                    dZ = dZ * dropout_mask / (1 - self.dropout_rates[i-1])\n",
    "\n",
    "            grads[f\"dW{i}\"] = np.dot(A_prev.T, dZ) / m\n",
    "            grads[f\"db{i}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "            if i > 1:  # Only if not at the input layer\n",
    "                dA = np.dot(dZ, W.T)\n",
    "\n",
    "            if self.regularizer == \"l2\":\n",
    "                grads[f\"dW{i}\"] += (self.lambda_reg / m) * W\n",
    "            elif self.regularizer == \"l1\":\n",
    "                grads[f\"dW{i}\"] += (self.lambda_reg / m) * np.sign(W)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, grads, optimizer):\n",
    "        for i in range(1, len(self.layers)):\n",
    "            W = self.params[f\"W{i}\"]\n",
    "            b = self.params[f\"b{i}\"]\n",
    "\n",
    "            self.params[f\"W{i}\"], self.params[f\"b{i}\"] = optimizer.update(grads[f\"dW{i}\"], self.params[f\"W{i}\"]), optimizer.update(grads[f\"db{i}\"], self.params[f\"b{i}\"])            \n",
    "        return self.params\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate, batch_size, optimizer=None, decay=0.0):\n",
    "        losses = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            Y_shuffled = Y[permutation]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch_start in range(0, n_samples, batch_size):\n",
    "                batch_end = min(batch_start + batch_size, n_samples)\n",
    "                X_batch = X_shuffled[batch_start:batch_end]\n",
    "                Y_batch = Y_shuffled[batch_start:batch_end]\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = self.forward(X_batch, training=True)\n",
    "                # Compute loss\n",
    "                batch_loss = self.compute_loss(Y_batch, predictions)\n",
    "                epoch_loss += batch_loss\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Backward pass and update\n",
    "                grads = self.backward(X_batch, Y_batch)\n",
    "                self.update_params(grads, optimizer)\n",
    "            \n",
    "            # Average loss for the epoch\n",
    "            epoch_loss /= batch_count\n",
    "            losses.append(epoch_loss)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            learning_rate *= (1 / (1 + decay * epoch))\n",
    "            \n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.forward(X, training=False)\n",
    "        return np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705ace7-6e5d-4dc6-8e29-7332ec34865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and reshape data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)  # Flatten to (50000, 3072)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)     # Flatten to (10000, 3072)\n",
    "\n",
    "# Convert labels to one-hot\n",
    "y_train = y_train.reshape(-1)  \n",
    "y_test = y_test.reshape(-1)    \n",
    "\n",
    "# Ensure labels are integer type\n",
    "y_train = np.eye(10)[y_train.astype(int)]  \n",
    "y_test = np.eye(10)[y_test.astype(int)]    \n",
    "\n",
    "# Check the shape of y_train after transformation\n",
    "print(\"y_train shape after one-hot encoding:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network\n",
    "nn = NeuralNetwork(\n",
    "    layers=[3072, 512, 256, 10],  \n",
    "    activations=['relu', 'relu', 'softmax'],\n",
    "    dropout_rates=[0.2, 0.2, 0.0],\n",
    "    regularizer='l2',\n",
    "    lambda_reg=0.01\n",
    ")\n",
    "\n",
    "optimizer = SGD(learning_rate=0.001)\n",
    "\n",
    "# Train\n",
    "losses = nn.train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    decay=1e-6,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe457a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    nn.forward(x_train, training=True)  \n",
    "    nn.backward(x_train, y_train)  \n",
    "    predictions = nn.cache[f'A{len(nn.layers)-1}']  # Final activations (predictions)\n",
    " \n",
    "    loss = np.mean(np.square(predictions - y_train))  # MSE loss\n",
    "    \n",
    "    losses.append(loss)\n",
    "    \n",
    "# Plotting the loss over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), losses, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780e526",
   "metadata": {},
   "source": [
    "# G.  Evaluate different neural network architectures/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad806d7-1b3c-4671-8ca4-e611419f45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_architectures():\n",
    "    return [\n",
    "        ([64], 'single hidden layer with 64 units'),\n",
    "        ([128, 64], 'two hidden layers with 128 and 64 units'),\n",
    "        ([256, 128, 64], 'three hidden layers with 256, 128, and 64 units')]\n",
    "\n",
    "def get_optimizers():\n",
    "    return [('SGD', SGD(learning_rate=0.01)),\n",
    "    ('SGD with Momentum', SGDWithMomentum(learning_rate=0.01, momentum=0.9))]\n",
    "\n",
    "def get_regularizations():\n",
    "    return [ ('Dropout 0.2', 0.2, None),  # Dropout rate of 0.2\n",
    "    ('Dropout 0.5', 0.5, None),  # Dropout rate of 0.5\n",
    "    ('L2 Regularization', None, 0.01)]  # L2 regularization with lambda = 0.01\n",
    "\n",
    "def get_hyperparameters():\n",
    "    return [\n",
    "        ('Learning Rate 0.001', 0.001, 32),\n",
    "        ('Learning Rate 0.005', 0.005, 64),\n",
    "        ('Learning Rate 0.01', 0.01, 128)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bafefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different hyperparameter configurations\n",
    "architectures = [\n",
    "    [3072, 512, 256, 10],  # Architecture A\n",
    "    [3072, 1024, 10],      # Architecture B\n",
    "    [3072, 1024, 512, 10]  # Architecture C\n",
    "]\n",
    "\n",
    "activations = [\n",
    "    ['relu', 'relu', 'softmax'],  # ReLU for hidden, Softmax for output\n",
    "    ['sigmoid', 'sigmoid', 'softmax']  # Sigmoid for hidden, Softmax for output\n",
    "]\n",
    "\n",
    "optimizers = [\n",
    "    SGD(learning_rate=0.001),\n",
    "]\n",
    "\n",
    "regularizations = [\n",
    "    ('l2', 0.01), \n",
    "    ('dropout', 0.2),  \n",
    "    ('none', 0)  \n",
    "]\n",
    "\n",
    "# Initialize variables to store loss values\n",
    "losses_dict = {}\n",
    "\n",
    "for architecture in architectures:\n",
    "    for activation in activations:\n",
    "        for optimizer in optimizers:\n",
    "            for reg_type, lambda_reg in regularizations:\n",
    "                \n",
    "                nn = NeuralNetwork(\n",
    "                    layers=architecture,\n",
    "                    activations=activation,\n",
    "                    regularizer=reg_type,\n",
    "                    lambda_reg=lambda_reg\n",
    "                )\n",
    "                \n",
    "                # Train the model\n",
    "                losses = nn.train(\n",
    "                    x_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    learning_rate=0.001,\n",
    "                    batch_size=32,\n",
    "                    decay=1e-6,\n",
    "                    optimizer=optimizer\n",
    "                )\n",
    "                \n",
    "                # Store the loss for each configuration\n",
    "                key = f\"Arch {architecture} - Act {activation} - Opt {optimizer.__class__.__name__} - Reg {reg_type}\"\n",
    "                losses_dict[key] = losses\n",
    "                \n",
    "                # Plot the loss for this configuration\n",
    "                plt.plot(losses, label=key)\n",
    "\n",
    "# Plot the loss curves for different configurations\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss for Different Architectures and Hyperparameters')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
