{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d320c960-a9c3-45ff-a465-5db2181c6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc42fac-dc94-4bd1-a610-cf1f59a14a14",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "## Sigmoid function\n",
    "The sigmoid function is used as an activation function because it squashes the output to a probability value between 0 and 1, which is useful when the output is a probability or binary; hence, it is commonly used in binary classification models. The function also allows the network to learn more complex decision bondaries. The formula for the sigmoid function is $$ σ(x) = \\frac{1}{1 + e^{-x}}. $$\n",
    "## Derivative of sigmoid\n",
    "Back propagation is essential to calculate the grandient of the loss function with respect to the weights and biases in a neural network. It allows the netowrk to effectively learn from its errors and adapt its weights based on the activating functions to update. The backward pass for sigmoid is the deravative of the sigmoid function, which can be mathematically expressed as $$ σ'(x) = σ(x) \\cdot \\bigl(1 - σ(x)\\bigr) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960b0abe-f743-437d-b006-c9e4ae625d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        return Sigmoid.forward(x) * (1 - Sigmoid.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95c1b2-5580-4f77-8a80-d411542ecbf2",
   "metadata": {},
   "source": [
    "## Tanh function\n",
    "The output for the tanh function is symmetric around the origin, which can help learning alorithms converge. This function outperforms the sigmoid function in multi-layer neural networks. The formula for the tanh function can be expressed as $$ tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\n",
    "## Derivative of tanh\n",
    "Similarly to the backward pass of the sigmoid function, the backward pass of the tanh function is the derivate of it, which can be expressed as $$tanh'(x){dx} = 1 - tanh(x)^{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3e2a9b-de82-488a-ada9-e06103d3047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        return 1 - (Tanh.forward(x) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2f8be-3737-42a7-84d9-40029ce93927",
   "metadata": {},
   "source": [
    "## ReLU function\n",
    "The ReLU (Rectified Linear Unit) function helps the model learn more complex relationships in data and makes accurate predictions, and it's computationally efficient, due to its non-linearity. The ReLU function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}(x) = \n",
    "\\begin{cases} \n",
    "x, & \\text{if } x \\geq 0 \\\\ \n",
    "0, & \\text{if } x < 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "## Derivative of ReLU\n",
    "The backward pass for the relu function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}'(x) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } x > 0 \\\\ \n",
    "0, & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba63e52-7deb-4056-b224-2cb5776ee796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return np.maximum(0, x)\n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8d8f3-dc1c-4fc1-8514-630440b64fb5",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "## mean Squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5661d388-baa2-4305-a029-30675a7bf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def forward(y_true, y_pred):\n",
    "        return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(y_true, y_pred):\n",
    "        return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d6d99-02bd-4e02-92a5-7f686e86662d",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "Unlike the sigmoid function, the softmax function is used in multiclass classification tasks: the function converts the output into probabilities, where the probability represents the likelihood of the input being in each class.The softmax function can mathematically be expressed as $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$\n",
    "## Derivative of Softmax\n",
    "$$\n",
    "softmax'(z_i) = \\text{softmax}(z_i) \\cdot (\\delta_{ik} - \\text{softmax}(z_k))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } \\delta_{ik} = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } i = k \\\\ \n",
    "0, & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a40bd61-edcb-44ee-8310-279531a085b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    @staticmethod\n",
    "    def forward(vector):\n",
    "        e = np.exp(vector)\n",
    "        return e / np.sum(e)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(vector, y):\n",
    "        p = softmax_forward(vector)\n",
    "        return p - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ce958-ab6a-4292-93d2-6b3df51220da",
   "metadata": {},
   "source": [
    "# Dropout function\n",
    "Dropout prevents overfitting and regularises by randomly \"dropping\" connections between neurons in successive layers when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "117dcb82-b5b2-414b-a5cb-ecdcc1a9d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, dropout_rate, training=True):\n",
    "    if training:\n",
    "        mask = np.random.rand(*X.shape) < (1 - dropout_rate)\n",
    "        X = X * mask / (1 - dropout_rate)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d6a00-2f36-4093-9992-63077fa6aa5c",
   "metadata": {},
   "source": [
    "# Implemented Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7974729b-d864-4385-a325-2683d23f834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8e2db1-c994-4612-b91d-5a41a351f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        if self.activation == \"sigmoid\":\n",
    "            self.output = Sigmoid.forward(self.input)\n",
    "        elif self.activation == \"tanh\":\n",
    "            self.output = Tanh.forward(self.input)\n",
    "        elif self.activation == \"relu\":\n",
    "            self.output = Relu.forward(self.input)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return Sigmoid.backward(self.input) * output_error\n",
    "        elif self.activation == \"tanh\":\n",
    "            return Tanh.backward(self.input) * output_error\n",
    "        elif self.activation == \"relu\":\n",
    "            return Relu.backward(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac0c93f-640b-481b-84a1-b955a05a362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, filter_size, stride=1, padding=0):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.1  # Initialize filters\n",
    "        self.biases = np.zeros(num_filters)  # Initialize biases\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        # Add padding if needed\n",
    "        if self.padding > 0:\n",
    "            input_data = np.pad(input_data, [(self.padding, self.padding), (self.padding, self.padding)], mode='constant')\n",
    "\n",
    "        self.input = input_data\n",
    "        input_height, input_width = input_data.shape\n",
    "        filter_height, filter_width = self.filters.shape[1], self.filters.shape[2]\n",
    "\n",
    "        # Calculate output dimensions\n",
    "        output_height = (input_height - filter_height) // self.stride + 1\n",
    "        output_width = (input_width - filter_width) // self.stride + 1\n",
    "\n",
    "        self.output = np.zeros((self.num_filters, output_height, output_width))\n",
    "\n",
    "        for f in range(self.num_filters):\n",
    "            for i in range(0, input_height - filter_height + 1, self.stride):\n",
    "                for j in range(0, input_width - filter_width + 1, self.stride):\n",
    "                    self.output[f, i // self.stride, j // self.stride] = np.sum(input_data[i:i + filter_height, j:j + filter_width] * self.filters[f]) + self.biases[f]\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        filter_height, filter_width = self.filters.shape[1], self.filters.shape[2]\n",
    "        input_height, input_width = self.input.shape\n",
    "        output_height, output_width = output_error.shape[1], output_error.shape[2]\n",
    "\n",
    "        # Initialize gradients\n",
    "        filter_gradient = np.zeros(self.filters.shape)\n",
    "        bias_gradient = np.zeros(self.biases.shape)\n",
    "        input_gradient = np.zeros(self.input.shape)\n",
    "\n",
    "        for f in range(self.num_filters):\n",
    "            for i in range(output_height):\n",
    "                for j in range(output_width):\n",
    "                    # Compute the gradient for the filter and bias\n",
    "                    region = self.input[i * self.stride:i * self.stride + filter_height, j * self.stride:j * self.stride + filter_width]\n",
    "                    filter_gradient[f] += region * output_error[f, i, j]\n",
    "                    bias_gradient[f] += output_error[f, i, j]\n",
    "\n",
    "                    # Compute the gradient for the input\n",
    "                    input_gradient[i * self.stride:i * self.stride + filter_height, j * self.stride:j * self.stride + filter_width] += self.filters[f] * output_error[f, i, j]\n",
    "\n",
    "        # Update filters and biases\n",
    "        self.filters -= learning_rate * filter_gradient\n",
    "        self.biases -= learning_rate * bias_gradient\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce1fa63-818d-4bcb-8d16-08bf340b4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        # input_data shape: (batch_size, channels, height, width)\n",
    "        self.input = input_data\n",
    "        batch_size, channels, input_height, input_width = input_data.shape\n",
    "\n",
    "        # Calculate output dimensions\n",
    "        output_height = (input_height - self.pool_size) // self.stride + 1\n",
    "        output_width = (input_width - self.pool_size) // self.stride + 1\n",
    "\n",
    "        # Output shape will be: (batch_size, channels, output_height, output_width)\n",
    "        self.output = np.zeros((batch_size, channels, output_height, output_width))\n",
    "        self.indices = np.zeros_like(self.output, dtype=int)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(0, input_height - self.pool_size + 1, self.stride):\n",
    "                    for j in range(0, input_width - self.pool_size + 1, self.stride):\n",
    "                        region = input_data[b, c, i:i + self.pool_size, j:j + self.pool_size]\n",
    "                        self.output[b, c, i // self.stride, j // self.stride] = np.max(region)\n",
    "                        self.indices[b, c, i // self.stride, j // self.stride] = np.argmax(region)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        batch_size, channels, output_height, output_width = output_error.shape\n",
    "        input_gradient = np.zeros(self.input.shape)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(output_height):\n",
    "                    for j in range(output_width):\n",
    "                        region_start_i = i * self.stride\n",
    "                        region_start_j = j * self.stride\n",
    "                        idx = self.indices[b, c, i, j]\n",
    "\n",
    "                        # Place the error in the correct location based on the max pool index\n",
    "                        input_gradient[b, c, region_start_i + idx // self.pool_size, region_start_j + idx % self.pool_size] += output_error[b, c, i, j]\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d046f2d-d873-4b66-9c6b-0be380f41d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input_shape = input_data.shape\n",
    "        self.output = input_data.flatten()\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07c039d8-361c-4de1-b7c3-c7f478f42f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, learning_rate, epochs):\n",
    "        dims = len(X)\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(dims):\n",
    "                output = X[j].reshape(1, -1)\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                \n",
    "                err += MSE.forward(y[j], output)\n",
    "    \n",
    "                # Backward pass\n",
    "                error = MSE.backward(y[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "                if j % 200 == 0: print(j)\n",
    "    \n",
    "            # Print the error at the end of each epoch\n",
    "            err /= dims\n",
    "            print(f\"Epoch {i+1}/{epochs} Error: {err:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, test):\n",
    "        dims = len(test)\n",
    "        results = []\n",
    "\n",
    "        for i in range(dims):\n",
    "            print(i)\n",
    "            output = test[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            results.append(output)\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f94697e-501a-45e2-8bff-90fc2f19a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8ecd0f2-8923-4601-b2c6-34df7b6e0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5ea08a9-556d-4a34-a349-b5725712be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0:\"airplane\",\n",
    "    1:\"automobile\",\n",
    "    2:\"bird\",\n",
    "    3:\"cat\",\n",
    "    4:\"deer\",\n",
    "    5:\"dog\",\n",
    "    6:\"frog\",\n",
    "    7:\"horse\",\n",
    "    8:\"ship\",\n",
    "    9:\"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25fc952-9bd9-4306-a42b-3cc11baff376",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test /255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccc45dc1-77c7-4dc0-8112-170196a930c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nahia\\AppData\\Local\\Temp\\ipykernel_20408\\3287217144.py:5: RuntimeWarning: overflow encountered in power\n",
      "  return np.mean(np.power(y_true-y_pred, 2));\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m net\u001b[38;5;241m.\u001b[39madd(ActivationLayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Training the network (fit)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m net\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 24\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self, X, y, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m     error \u001b[38;5;241m=\u001b[39m MSE\u001b[38;5;241m.\u001b[39mbackward(y[j], output)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 24\u001b[0m         error \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mbackward_propagation(error, learning_rate)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mprint\u001b[39m(j)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Print the error at the end of each epoch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36mLayer.backward_propagation\u001b[1;34m(self, output_error, learning_rate)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_propagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_error, learning_rate):\n\u001b[0;32m     12\u001b[0m     input_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(output_error, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m---> 13\u001b[0m     weights_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mT, output_error)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m weights_error\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m output_error\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a network instance\n",
    "net = NeuralNetwork()\n",
    "\n",
    "# Add convolutional layers, max pooling, and flattening\n",
    "net.add(Layer(3072, 1024))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(1024, 256))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(256, 64))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(64, 10))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "\n",
    "# Training the network (fit)\n",
    "net.fit(x_train, y_train, epochs=24, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443439e8-f378-450b-995b-b30f9ba3a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(x_test.flatten())\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
