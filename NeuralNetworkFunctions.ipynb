{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d320c960-a9c3-45ff-a465-5db2181c6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc42fac-dc94-4bd1-a610-cf1f59a14a14",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "## Sigmoid function\n",
    "The sigmoid function is used as an activation function because it squashes the output to a probability value between 0 and 1, which is useful when the output is a probability or binary; hence, it is commonly used in binary classification models. The function also allows the network to learn more complex decision bondaries. The formula for the sigmoid function is $$ σ(x) = \\frac{1}{1 + e^{-x}}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "960b0abe-f743-437d-b006-c9e4ae625d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db330738-04ae-4125-8085-2e9d0324a8a6",
   "metadata": {},
   "source": [
    "## Derivative of sigmoid\n",
    "Back propagation is essential to calculate the grandient of the loss function with respect to the weights and biases in a neural network. It allows the netowrk to effectively learn from its errors and adapt its weights based on the activating functions to update. The backward pass for sigmoid is the deravative of the sigmoid function, which can be mathematically expressed as $$ \\frac{\\mathrm{d}σ}{\\mathrm{d}x}(x) = σ(x) \\cdot \\bigl(1 - σ(x)\\bigr) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "819e93d4-b645-4d58-a34d-21e34a1aca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(x):\n",
    "    return sigmoid_forward(x) * (1 - sigmoid_forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95c1b2-5580-4f77-8a80-d411542ecbf2",
   "metadata": {},
   "source": [
    "## Tanh function\n",
    "The output for the tanh function is symmetric around the origin, which can help learning alorithms converge. This function outperforms the sigmoid function in multi-layer neural networks. The formula for the tanh function can be expressed as $$ tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e2a9b-de82-488a-ada9-e06103d3047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18c035-fab3-4e3e-ab8d-42ba44e77e80",
   "metadata": {},
   "source": [
    "Similarly to the backward pass of the sigmoid function, the backward pass of the tanh function is the derivate of it, which can be expressed as $$ \\frac{d(tanh)}{dx} = 1 - tanh(x)^{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e143c-65fb-4e76-b7ad-779b0bcbf7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_tanh(x):\n",
    "    return 1 - (forward_tanh(x) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2f8be-3737-42a7-84d9-40029ce93927",
   "metadata": {},
   "source": [
    "# ReLU function\n",
    "The ReLU (Rectified Linear Unit) function helps the model learn more complex relationships in data and makes accurate predictions, and it's computationally efficient, due to its non-linearity. The ReLU function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}(x) = \n",
    "\\begin{cases} \n",
    "x, & \\text{if } x \\geq 0 \\\\ \n",
    "0, & \\text{if } x < 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cba63e52-7deb-4056-b224-2cb5776ee796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb665b-9add-4866-853a-f0bb2da886e1",
   "metadata": {},
   "source": [
    "The backward pass for the relu function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}'(x) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } x > 0 \\\\ \n",
    "0, & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69be99f4-9b52-45be-b8b2-c134cde23ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(x):\n",
    "    return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d6d99-02bd-4e02-92a5-7f686e86662d",
   "metadata": {},
   "source": [
    "# Softmax function\n",
    "Unlike the sigmoid function, the softmax function is used in multiclass classification tasks: the function converts the output into probabilities, where the probability represents the likelihood of the input being in each class.The softmax function can mathematically be expressed as $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a40bd61-edcb-44ee-8310-279531a085b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / np.sum(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471aa34-a9b1-42d3-99af-63db4c5c5088",
   "metadata": {},
   "source": [
    "$$\n",
    "'(z_i)} = \\text{softmax}(z_i) \\cdot (\\delta_{ik} - \\text{softmax}(z_k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } \\delta_{ik} = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } i = k \\\\ \n",
    "0, & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b93ea4db-4599-47fa-80cf-25e6d998d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(vector, y):\n",
    "    p = softmax_forward(vector)\n",
    "    return p - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ce958-ab6a-4292-93d2-6b3df51220da",
   "metadata": {},
   "source": [
    "# Dropout function\n",
    "Dropout prevents overfitting and regularises by randomly \"dropping\" connections between neurons in successive layers when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117dcb82-b5b2-414b-a5cb-ecdcc1a9d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, dropout_rate, training=True):\n",
    "    if training:\n",
    "        mask = np.random.rand(*X.shape) < (1 - dropout_rate)\n",
    "        X = X * mask / (1 - dropout_rate)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d6a00-2f36-4093-9992-63077fa6aa5c",
   "metadata": {},
   "source": [
    "# Implemented Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07c039d8-361c-4de1-b7c3-c7f478f42f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sigmoid_forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_backward(self, x):\n",
    "        return sigmoid_forward(x) * (1 - sigmoid_forward(x))\n",
    "\n",
    "    def relu_forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_backward(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    def softmax_forward(self, vector):\n",
    "        e = np.exp(vector)\n",
    "        return e / np.sum(e)\n",
    "\n",
    "    def softmax_backward(self, vector, y):\n",
    "        p = softmax_forward(vector)\n",
    "        return p - y\n",
    "\n",
    "    def dropout(X, dropout_rate, training=True):\n",
    "        if training:\n",
    "            mask = np.random.rand(*X.shape) < (1 - dropout_rate)\n",
    "            X = X * mask / (1 - dropout_rate)\n",
    "        return X\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
