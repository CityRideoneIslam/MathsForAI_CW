{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d320c960-a9c3-45ff-a465-5db2181c6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc42fac-dc94-4bd1-a610-cf1f59a14a14",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "## Sigmoid function\n",
    "The sigmoid function is used as an activation function because it squashes the output to a probability value between 0 and 1, which is useful when the output is a probability or binary; hence, it is commonly used in binary classification models. The function also allows the network to learn more complex decision bondaries. The formula for the sigmoid function is $$ σ(x) = \\frac{1}{1 + e^{-x}}. $$\n",
    "## Derivative of sigmoid\n",
    "Back propagation is essential to calculate the grandient of the loss function with respect to the weights and biases in a neural network. It allows the netowrk to effectively learn from its errors and adapt its weights based on the activating functions to update. The backward pass for sigmoid is the deravative of the sigmoid function, which can be mathematically expressed as $$ σ'(x) = σ(x) \\cdot \\bigl(1 - σ(x)\\bigr) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960b0abe-f743-437d-b006-c9e4ae625d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def backward(x):\n",
    "        return sigmoid_forward(x) * (1 - sigmoid_forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95c1b2-5580-4f77-8a80-d411542ecbf2",
   "metadata": {},
   "source": [
    "## Tanh function\n",
    "The output for the tanh function is symmetric around the origin, which can help learning alorithms converge. This function outperforms the sigmoid function in multi-layer neural networks. The formula for the tanh function can be expressed as $$ tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\n",
    "## Derivative of tanh\n",
    "Similarly to the backward pass of the sigmoid function, the backward pass of the tanh function is the derivate of it, which can be expressed as $$tanh'(x){dx} = 1 - tanh(x)^{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3e2a9b-de82-488a-ada9-e06103d3047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    \n",
    "    def backward(x):\n",
    "        return 1 - (forward_tanh(x) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2f8be-3737-42a7-84d9-40029ce93927",
   "metadata": {},
   "source": [
    "## ReLU function\n",
    "The ReLU (Rectified Linear Unit) function helps the model learn more complex relationships in data and makes accurate predictions, and it's computationally efficient, due to its non-linearity. The ReLU function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}(x) = \n",
    "\\begin{cases} \n",
    "x, & \\text{if } x \\geq 0 \\\\ \n",
    "0, & \\text{if } x < 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "## Derivative of ReLU\n",
    "The backward pass for the relu function can be expressed as \n",
    "$$\n",
    "\\text{ReLU}'(x) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } x > 0 \\\\ \n",
    "0, & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba63e52-7deb-4056-b224-2cb5776ee796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def forward(x):\n",
    "        return np.maximum(0, x)\n",
    "        \n",
    "    def backward(x):\n",
    "        return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8d8f3-dc1c-4fc1-8514-630440b64fb5",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "## mean Squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5661d388-baa2-4305-a029-30675a7bf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "class MSE:\n",
    "    def forward(self, y_true, y_pred):\n",
    "        return np.mean(np.power(y_true-y_pred, 2));\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d6d99-02bd-4e02-92a5-7f686e86662d",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "Unlike the sigmoid function, the softmax function is used in multiclass classification tasks: the function converts the output into probabilities, where the probability represents the likelihood of the input being in each class.The softmax function can mathematically be expressed as $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$\n",
    "## Derivative of Softmax\n",
    "$$\n",
    "softmax'(z_i) = \\text{softmax}(z_i) \\cdot (\\delta_{ik} - \\text{softmax}(z_k))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } \\delta_{ik} = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } i = k \\\\ \n",
    "0, & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a40bd61-edcb-44ee-8310-279531a085b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def softmax_forward(vector):\n",
    "        e = np.exp(vector)\n",
    "        return e / np.sum(e)\n",
    "    def softmax_backward(vector, y):\n",
    "        p = softmax_forward(vector)\n",
    "        return p - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ce958-ab6a-4292-93d2-6b3df51220da",
   "metadata": {},
   "source": [
    "# Dropout function\n",
    "Dropout prevents overfitting and regularises by randomly \"dropping\" connections between neurons in successive layers when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "117dcb82-b5b2-414b-a5cb-ecdcc1a9d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, dropout_rate, training=True):\n",
    "    if training:\n",
    "        mask = np.random.rand(*X.shape) < (1 - dropout_rate)\n",
    "        X = X * mask / (1 - dropout_rate)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d6a00-2f36-4093-9992-63077fa6aa5c",
   "metadata": {},
   "source": [
    "# Implemented Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7974729b-d864-4385-a325-2683d23f834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c8e2db1-c994-4612-b91d-5a41a351f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        if self.activation == \"sigmoid\":\n",
    "            self.output = Sigmoid.forward(self.input)\n",
    "        elif self.activation == \"tanh\":\n",
    "            self.output = Tanh.forward(self.input)\n",
    "        elif self.activation == \"relu\":\n",
    "            self.output = Relu.forward(self.input)\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return Sigmoid.backward(self.input) * output_error\n",
    "        elif self.activation == \"tanh\":\n",
    "            return Tanh.backward(self.input) * output_error\n",
    "        elif self.activation == \"relu\":\n",
    "            return Relu.backward(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "366b1abe-4bf2-4269-b227-03633eec1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DLayer:\n",
    "    def __init__(self, num_filters, kernel_size, input_shape):\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.filters = np.random.randn(num_filters, kernel_size, kernel_size, input_shape[2]) / kernel_size**2\n",
    "        self.biases = np.zeros((num_filters, 1))\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        print(input_data)\n",
    "        h, w, c = input_data.shape\n",
    "        out_h = h - self.kernel_size + 1\n",
    "        out_w = w - self.kernel_size + 1\n",
    "        self.output = np.zeros((out_h, out_w, self.num_filters))\n",
    "\n",
    "        for f in range(self.num_filters):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    region = input_data[i:i+self.kernel_size, j:j+self.kernel_size, :]\n",
    "                    self.output[i, j, f] = np.sum(region * self.filters[f]) + self.biases[f]\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        d_filters = np.zeros_like(self.filters)\n",
    "        d_biases = np.zeros_like(self.biases)\n",
    "        d_input = np.zeros_like(self.input)\n",
    "\n",
    "        out_h, out_w, _ = output_error.shape\n",
    "        for f in range(self.num_filters):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    region = self.input[i:i+self.kernel_size, j:j+self.kernel_size, :]\n",
    "                    d_filters[f] += output_error[i, j, f] * region\n",
    "                    d_input[i:i+self.kernel_size, j:j+self.kernel_size, :] += output_error[i, j, f] * self.filters[f]\n",
    "            d_biases[f] = np.sum(output_error[:, :, f])\n",
    "\n",
    "        self.filters -= learning_rate * d_filters\n",
    "        self.biases -= learning_rate * d_biases\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1f7ce1d6-f3f2-426a-9070-4643f79e8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        print(input_data)\n",
    "        h, w = input_data.shape\n",
    "        out_h = h // self.pool_size\n",
    "        out_w = w // self.pool_size\n",
    "        self.output = np.zeros((out_h, out_w))\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                region = input_data[i*self.pool_size:(i+1)*self.pool_size,\n",
    "                                    j*self.pool_size:(j+1)*self.pool_size]\n",
    "                self.output[i, j] = np.max(region)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        d_input = np.zeros_like(self.input)\n",
    "        out_h, out_w = output_error.shape\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                region = self.input[i*self.pool_size:(i+1)*self.pool_size,\n",
    "                                    j*self.pool_size:(j+1)*self.pool_size]\n",
    "                max_val = np.max(region)\n",
    "                for x in range(self.pool_size):\n",
    "                    for y in range(self.pool_size):\n",
    "                        if region[x, y] == max_val:\n",
    "                            d_input[i*self.pool_size + x, j*self.pool_size + y] = output_error[i, j]\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "81ee3bf9-0182-4623-b5a6-6c450d2e6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input_shape = input_data.shape\n",
    "        return input_data.flatten()\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error.reshape(self.input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "07c039d8-361c-4de1-b7c3-c7f478f42f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, learning_rate, epochs):\n",
    "        dims = len(X)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(dims):\n",
    "                output = X[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                err += MSE.forward(y[j], output)\n",
    "\n",
    "                error += MSE.backward(y[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "                err /= dims\n",
    "                print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "\n",
    "    def predict(self, test):\n",
    "        dims = len(test)\n",
    "        results = []\n",
    "\n",
    "        for i in range(dims):\n",
    "            output = test[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            results.append(output)\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3f94697e-501a-45e2-8bff-90fc2f19a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d8ecd0f2-8923-4601-b2c6-34df7b6e0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5ea08a9-556d-4a34-a349-b5725712be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0:\"airplane\",\n",
    "    1:\"automobile\",\n",
    "    2:\"bird\",\n",
    "    3:\"cat\",\n",
    "    4:\"deer\",\n",
    "    5:\"dog\",\n",
    "    6:\"frog\",\n",
    "    7:\"horse\",\n",
    "    8:\"ship\",\n",
    "    9:\"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c25fc952-9bd9-4306-a42b-3cc11baff376",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test /255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "89f456fe-aa5c-4dbd-aa8f-f7c645d3d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([i.flatten() for i in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1bd2e248-767a-41b5-8dfc-f298df56ca90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ccc45dc1-77c7-4dc0-8112-170196a930c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m nn\u001b[38;5;241m.\u001b[39madd(ActivationLayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m nn\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[122], line 18\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self, X, y, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m X[j]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 18\u001b[0m     output \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward_propagation(output)\n\u001b[0;32m     20\u001b[0m err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m MSE\u001b[38;5;241m.\u001b[39mforward(y[j], output)\n\u001b[0;32m     22\u001b[0m error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m MSE\u001b[38;5;241m.\u001b[39mbackward(y[j], output)\n",
      "Cell \u001b[1;32mIn[156], line 11\u001b[0m, in \u001b[0;36mConv2DLayer.forward_propagation\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m input_data\n\u001b[1;32m---> 11\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     12\u001b[0m     out_h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     13\u001b[0m     out_w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork()\n",
    "\n",
    "net.add(Layer(784, 512))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(512, 256))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(256, 128))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(128, 64))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(64, 32))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(32, 16))\n",
    "net.add(ActivationLayer(\"relu\"))\n",
    "net.add(Layer(16, 10))\n",
    "\n",
    "net.fit(x, y_train, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ec681243-9f96-429f-ac6c-9f4379a92e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.2918 - loss: 1.8766\n",
      "Epoch 2/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5134 - loss: 1.3344\n",
      "Epoch 3/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5785 - loss: 1.1750\n",
      "Epoch 4/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6241 - loss: 1.0550\n",
      "Epoch 5/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6487 - loss: 0.9889\n",
      "Epoch 6/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6782 - loss: 0.9150\n",
      "Epoch 7/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6982 - loss: 0.8546\n",
      "Epoch 8/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7119 - loss: 0.8181\n",
      "Epoch 9/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7236 - loss: 0.7792\n",
      "Epoch 10/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7426 - loss: 0.7352\n",
      "Epoch 11/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7499 - loss: 0.7055\n",
      "Epoch 12/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7582 - loss: 0.6852\n",
      "Epoch 13/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7633 - loss: 0.6633\n",
      "Epoch 14/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7769 - loss: 0.6313\n",
      "Epoch 15/15\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7848 - loss: 0.6061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x11de1e04530>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
